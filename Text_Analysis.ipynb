{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis\n",
    "\n",
    "In this module, we will use the Natural Language Toolkit Library (NLTK) to look at individual words and sentences in a text and clean unneccessary features from the text data to prepare for sentiment analysis. Then using the textblob library, we will analyze the sentiment of opinioned data to give a numerical value for use in a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing Words and Sentences\n",
    "\n",
    "Recall in the \"Python Dictionaries and String Manipulation\" notebook, we used the .split() function to break a sentence apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"My favorite color is purple\"\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, because the default character to split on is a space, the .split() function does not work well with sentences that have punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the comma is attached to the previous word\n",
    "text2 = \"My favorite foods are french fries, bacon, and cheese\"\n",
    "text2.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK library was built to separate punctuation from words when tokenizing (splitting into parts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#this is sample data\n",
    "from nltk.corpus import names  \n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "#if the next cell does not work\n",
    "#remove number symbol on following lines and re-run this cell\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the comma is now its own token when the sentence is split\n",
    "word_tokenize(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#muti-sentence texts can be tokenized by sentence\n",
    "#each sentence is an item in the list\n",
    "text3 = \"My name is Kenisha Priester. My favorite color is purple. My favorite foods are french fries, bacon, and cheese.\"\n",
    "\n",
    "sent_tokenize(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of social media data is in the form of tweets. A tweet can have an @ sign to tag another user or a # sign to tag a particular subject. When analyzing data, you may want to retain these symbols with the words/phrase they're attached to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets are more difficult to use the word tokenize function on\n",
    "#using word_tokenize, common social media signs get separated\n",
    "tweet = \"@animegurl OMG you are so #funny :P\"\n",
    "\n",
    "word_tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeps the @ sign, # sign, and the tongue-out emoji\n",
    "TweetTokenizer().tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember the Word Count exercise text?\n",
    "\n",
    "Let's clean it up using NLTK and do a basic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#work count exercise paragraph\n",
    "wctext = '''\n",
    "A CerTaiN kING HaD a bEaUtIFul gaRDEn, ANd IN THe GarDen StooD A trEe\n",
    "whICH bORe GoLDeN ApPlEs. THesE aPples WerE alwAyS CoUntEd, aNd abOuT\n",
    "tHE TiMe WheN tHEY BEgAn tO grow RipE IT wAs foUND THat EVeRY NIgHT ONE\n",
    "OF THeM Was gOne. thE kiNg bECAMe veRy ANGRy at thiS, aND ORDEred the\n",
    "GarDEneR TO KEeP WAtch ALL NIgHT uNDER the tREE. tHE gardener sEt hIs\n",
    "ELdEsT SoN to WATCH; buT ABout TweLve O'clOCK He fELL ASlEEp, And in\n",
    "the morNIng aNOTheR Of thE aPPLes Was mIssinG. tHEn THE sECONd Son waS\n",
    "oRdERED to waTch; aNd AT mIDniGhT he tOO FELl ASleEP, aND iN thE mOrNIng\n",
    "ANoThER AppLE WaS gOne . TheN THe thIrd Son oFfeREd tO KeEp wATCh; buT\n",
    "thE garDENer At First WoULd NoT LET Him, FOr fEaR sOMe HArM ShOuLD cOME\n",
    "To hIM: hoWeveR, at lAST hE coNSEnteD, AND tHE YouNg MAN laID HimSELf\n",
    "uNDER tHE tREe TO wAtch. AS tHE clocK STRuCk tweLvE He Heard A rustlinG\n",
    "NoISe IN ThE aIr, And a biRd CAME FlYing ThAt was Of PUre gOLd; AND as\n",
    "IT WAs SNApPING At onE oF ThE aPpleS wIth iTS BeaK, tHE GArDEner’S son\n",
    "jUMpED UP And SHOT AN aRrOw at iT. But THE arrOw DID thE BiRD nO HaRm;\n",
    "ONlY iT dRoPPEd a GoLDEn FEather FROM iTS tAiL, aND THEN FLEw AwaY.\n",
    "the gOLdEN FEAthER WAS bRoUght to THe KinG IN THE MOrNING, AnD aLL ThE\n",
    "cOunCil WAs called TogETHER. EVERYoNE aGREed ThAt it wAS wORth MoRe THAn\n",
    "aLl The weAltH Of tHE kIngDOm: But THE KiNg sAID, ‘One FeatHeR Is Of NO\n",
    "use tO me, I MusT HaVE ThE wHOLE BIRD .’\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, change all the words to lowercase\n",
    "wctext = wctext.lower()\n",
    "\n",
    "#then tokenize each part of the text\n",
    "tknz_wct = word_tokenize(wctext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknz_wct[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the NLTK FreqDist gives a count for how often each part of the text occurs\n",
    "fd_wct = FreqDist(tknz_wct)\n",
    "fd_wct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows the top 10 words in the text\n",
    "fd_wct.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common parts of this text seem to be filler words and punctuation. We need to remove them to get a better understand of what the text is about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of tokens in list before puntuation removal\n",
    "len(tknz_wct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the puntuation tokens from the list\n",
    "for word in tknz_wct:\n",
    "    if word in punctuation:\n",
    "        tknz_wct.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of tokens in list after puntuation removal\n",
    "len(tknz_wct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of english stopwords\n",
    "eng_stopwords = stopwords.words('english')\n",
    "eng_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_count = 0\n",
    "new_words = []  #list to hold new words\n",
    "\n",
    "for word in tknz_wct:\n",
    "    if word not in eng_stopwords:\n",
    "        new_words.append(word)\n",
    "    else: rm_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the new top 10 words in this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_nw = FreqDist(new_words)\n",
    "fd_nw.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words in Enlgish can change their form depending on if it's past tense, present tense, or future tense. We can reduce these words to their dictionary form so that it's easier for the computer to interpret the words. This is called **lemmatization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the word lemmatization function into a variable\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this sentence contains words with different tenses\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the sentence into a list\n",
    "#this is before we lemmatize it\n",
    "non_lem = word_tokenize(sentence)\n",
    "non_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty list to hold the new lemmatized words\n",
    "lemm = []\n",
    "\n",
    "for word in non_lem:\n",
    "    lemm.append(wnl.lemmatize(word, pos=\"v\"))  #lemmatize using 'verb' part-of-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the list of tokens after being lemmatized\n",
    "lemm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual analysis of last letter of male and female names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are two text files within the Names sample data\n",
    "names.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the words in the text to lists\n",
    "m_names = names.words('male.txt')\n",
    "f_names = names.words('female.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample the first 5 items in m_names\n",
    "m_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a frequency distribution of names that end with a particular letter (by gender)\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "            (fileid, name[-1])\n",
    "            for fileid in names.fileids()\n",
    "            for name in names.words(fileid))\n",
    "\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "In order to understand how people feel about something, we need to do sentiment analysis on text data that contains their opinion.\n",
    "\n",
    "You will need to [install the textblob library](https://anaconda.org/conda-forge/textblob)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myday = \"Today is a great day, but it is boring\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = TextBlob(myday)\n",
    "\n",
    "tb.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_pol = tb.sentiment.polarity\n",
    "type(tb_pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a sentiment value column in a dataframe\n",
    "\n",
    "Using the [Amazon Book Reviews dataset on Kaggle](https://www.kaggle.com/shrutimehta/amazon-book-reviews-webscraped), we add a new column to the dataset that will have a numerical value for the sentiment of each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#load the data from the Reviews.csv file\n",
    "filepath = \"Reviews.csv\"\n",
    "df = pd.read_csv(filepath, encoding = \"latin-1\") #this file is encoded differently\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to clean up each review\n",
    "#then it will analyze and assign a sentiment polarity\n",
    "def reviewSentiment(review):\n",
    "    \n",
    "    #make text lowercase\n",
    "    review = review.lower()\n",
    "    \n",
    "    #tokenize the review\n",
    "    tknz_review = word_tokenize(review)\n",
    "    \n",
    "    #remove puntuation\n",
    "    for token in tknz_review:\n",
    "        if token in punctuation:\n",
    "            tknz_review.remove(token)\n",
    "    \n",
    "    clean_tokens = []\n",
    "    #remove filler words\n",
    "    for token in tknz_review:\n",
    "        if token not in eng_stopwords:\n",
    "            clean_tokens.append(token)\n",
    "            \n",
    "    #put sentence back together with remaining clean words\n",
    "    clean_review = ' '.join(clean_tokens)\n",
    "    \n",
    "    #turn into textblob\n",
    "    blob_rev = TextBlob(clean_review)\n",
    "    \n",
    "    #get sentiment polarity\n",
    "    r_pol = blob_rev.sentiment.polarity\n",
    "    \n",
    "    return r_pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column to hold sentiment value from function\n",
    "df['review_sentiment'] = df['ReviewContent'].apply(reviewSentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#erify sentiment values in new column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to assign a polarity category to the sentiment\n",
    "def sentimentCategory(sent_num):\n",
    "    if sent_num >= 0.2:\n",
    "        return \"positive\"\n",
    "    if sent_num <= -0.2:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column to hold sentiment category\n",
    "df['sentiment_category'] = df['review_sentiment'].apply(sentimentCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare frequency of positive, negative, and neutral reviews\n",
    "df['sentiment_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, it seems that most readers feel so-so about the book (maybe some good parts and some bad parts) and some readers really like the book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
